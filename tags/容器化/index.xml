<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>容器化 on HNBCAO</title>
    <link>https://hnbcao.vip/tags/%E5%AE%B9%E5%99%A8%E5%8C%96/</link>
    <description>Recent content in 容器化 on HNBCAO</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zn-ch</language>
    <lastBuildDate>Tue, 16 Jun 2020 14:16:38 +0000</lastBuildDate><atom:link href="https://hnbcao.vip/tags/%E5%AE%B9%E5%99%A8%E5%8C%96/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Kubernetes集群安装</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/1-kubernetes-cluster-installation/</link>
      <pubDate>Tue, 31 Dec 2019 16:16:02 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/1-kubernetes-cluster-installation/</guid>
      <description>Kubernetes集群搭建 #  1.1. 概述 #   本文基于kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora编写，修改了master之间的负载均衡方式为HAProxy+keeplived方式。 此离线教程必须保证目标安装环境与离线包下载环境一致，或者是考虑做yum镜像源。 关于keepalived+haproxy负载均衡，由于是在阿里云上搭建的，事实上是没有实现的，至于为何也成功部署了环境，其实是每台机器上keepalived都处于激活状态，对虚拟ip的访问都映射到了本机，本机又通过haproxy将请求负载到了api-server上。这是个神奇的事情，直到现在才搞清楚keepalived+haproxy的原理，如果是在阿里云上部署，这块建议使用阿里云的负载均衡功能。（keepalived+haproxy是为了实现api-server的负载均衡） 关于内核，实际上升不升级应该问题都不是很大，至少目前环境没出现过问题。 关于kubernetes版本，目前该教程能支持最新的v1.15.3版本的安装，注意修改版本号。  集群方案：
 发行版：CentOS 7 容器运行时 内核： 4.18.12-1.el7.elrepo.x86_64 版本：Kubernetes: 1.14.0 网络方案: Calico kube-proxy mode: IPVS master高可用方案：HAProxy keepalived LVS DNS插件: CoreDNS metrics插件：metrics-server 界面：kubernetes-dashboard  1.2. 安装环境 #     Host Name Role IP     master1 master1 192.168.56.103   master2 master2 192.168.56.104   master3 master3 192.</description>
    </item>
    
    <item>
      <title>1. Kustomize声明式管理</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/advance/1-kubernetes-kustomize/</link>
      <pubDate>Tue, 31 Dec 2019 16:16:02 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/advance/1-kubernetes-kustomize/</guid>
      <description>Kustomize声明式管理 #  1.1. 简介 #  本文来自于官方文档
Kustomize 是一个独立的工具，用来通过 kustomization 文件 定制 Kubernetes 对象。它提供以下功能特性来管理 应用配置文件：
 从其他来源生成资源 为资源设置贯穿性（Cross-Cutting）字段 组织和定制资源集合  从 1.14 版本开始，kubectl 也开始支持使用 kustomization 文件来管理 Kubernetes 对象。 要查看包含 kustomization 文件的目录中的资源，执行下面的命令：
kubectl kustomize &amp;lt;kustomization_directory&amp;gt; 要应用这些资源，使用参数 &amp;ndash;kustomize 或 -k 标志来执行 kubectl apply：
kubectl apply -k &amp;lt;kustomization_directory&amp;gt; 1.2. 资源配置 #  1.2.1. configMapGenerator #  要生成 ConfigMap，可以在 configMapGenerator 中添加对应的表项，主要有files、envs以及literals。
具体示例如下（通过kubectl kustomize查看生成结果）：
 根据文件中的数据生成ConfigMap  cat &amp;lt;&amp;lt;EOF &amp;gt;tempconfig.properties VAR01=01 VAR02=02 EOF cat &amp;lt;&amp;lt;EOF &amp;gt;kustomization.</description>
    </item>
    
    <item>
      <title>1. Kustomize声明式管理</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/advance/2-istio-introduce/</link>
      <pubDate>Tue, 31 Dec 2019 16:16:02 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/advance/2-istio-introduce/</guid>
      <description>Kustomize声明式管理 #  1.1. 简介 #  本文来自于官方文档
Kustomize 是一个独立的工具，用来通过 kustomization 文件 定制 Kubernetes 对象。它提供以下功能特性来管理 应用配置文件：
 从其他来源生成资源 为资源设置贯穿性（Cross-Cutting）字段 组织和定制资源集合  从 1.14 版本开始，kubectl 也开始支持使用 kustomization 文件来管理 Kubernetes 对象。 要查看包含 kustomization 文件的目录中的资源，执行下面的命令：
kubectl kustomize &amp;lt;kustomization_directory&amp;gt; 要应用这些资源，使用参数 &amp;ndash;kustomize 或 -k 标志来执行 kubectl apply：
kubectl apply -k &amp;lt;kustomization_directory&amp;gt; 1.2. 资源配置 #  1.2.1. configMapGenerator #  要生成 ConfigMap，可以在 configMapGenerator 中添加对应的表项，主要有files、envs以及literals。
具体示例如下（通过kubectl kustomize查看生成结果）：
 根据文件中的数据生成ConfigMap  cat &amp;lt;&amp;lt;EOF &amp;gt;tempconfig.properties VAR01=01 VAR02=02 EOF cat &amp;lt;&amp;lt;EOF &amp;gt;kustomization.</description>
    </item>
    
    <item>
      <title>2. Kubernetes集群安装Cert Manager</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/2-kubernetes-cert-manager/</link>
      <pubDate>Tue, 31 Dec 2019 17:31:17 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/2-kubernetes-cert-manager/</guid>
      <description>安装Cert Manager #  2.1. 安装 #  https://cert-manager.io/docs/installation/kubernetes/
2.2. 创建ClusterIssuer #  集群内所有命名空间公用方案
apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: cluster-letsencrypt-prod spec: acme: email: hnbcao@qq.com privateKeySecretRef: name: cluster-letsencrypt-prod server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: traefik 2.3. Ingress应用ClusterIssuer #  kind: Ingress apiVersion: extensions/v1beta1 metadata: name: harbor-ingress namespace: ns-harbor labels: app: harbor chart: harbor heritage: Helm release: harbor annotations: cert-manager.io/cluster-issuer: cluster-letsencrypt-prod spec: tls: - hosts: - harbor.domian.io secretName: harbor-letsencrypt-tls rules: - host: harbor.</description>
    </item>
    
    <item>
      <title>3. Kubernetes集群安装Traefik Ingress</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/3-kubernetes-traefik-ingress/</link>
      <pubDate>Tue, 31 Dec 2019 17:21:39 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/3-kubernetes-traefik-ingress/</guid>
      <description>部署TraefikIngress #  3.1. 创建证书 #  使用OpenSSL创建TLS证书（已有证书则跳过该选项）
 设置证书信息  cd ~ &amp;amp;&amp;amp; mkdir tls echo &amp;#34;&amp;#34;&amp;#34; [req] distinguished_name = req_distinguished_name prompt = yes [ req_distinguished_name ] countryName = Country Name (2 letter code) countryName_value = CN stateOrProvinceName = State or Province Name (full name) stateOrProvinceName_value = Chongqing localityName = Locality Name (eg, city) localityName_value = Yubei organizationName = Organization Name (eg, company) organizationName_value = HNBCAO organizationalUnitName = Organizational Unit Name (eg, section) organizationalUnitName_value = R &amp;amp; D Department commonName = Common Name (eg, your name or your server\&amp;#39;s hostname) commonName_value = *.</description>
    </item>
    
    <item>
      <title>4. Kubernetes集群创建用户</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/4-create-kubernetes-user/</link>
      <pubDate>Tue, 31 Dec 2019 17:27:30 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/4-create-kubernetes-user/</guid>
      <description>创建集群用户 #  4.1. 创建用户 #  apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user annotations: rbac.authorization.kubernetes.io/autoupdate: &amp;#34;true&amp;#34; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system 4.2. 获取Token #  获取管理员用户的Token，通过执行如下命令获取系统Token信息
kubectl describe secret admin-user --namespace=kube-system 4.3. 导出配置 #  DASH_TOCKEN=$(kubectl get secret -n kube-system admin-user-token-4j272 -o jsonpath={.data.token}|base64 -d) kubectl config set-cluster kubernetes --server=https://172.16.0.9:8443 --kubeconfig=/root/kube-admin.</description>
    </item>
    
    <item>
      <title>5. Kubernetes集群创建Image Pull Secret</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/5-kubernetes-create-image-pull-secret/</link>
      <pubDate>Tue, 31 Dec 2019 17:23:44 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/5-kubernetes-create-image-pull-secret/</guid>
      <description>创建ImagePullSecret #  5.1. 登录仓库 #  登录镜像仓库，成功之后会生成如下/root/.docker/config.json文件
{ &amp;#34;auths&amp;#34;: { &amp;#34;harbor.hnbcao.tech&amp;#34;: { &amp;#34;auth&amp;#34;: &amp;#34;YWRtaW4******lRlY2g=&amp;#34; } }, &amp;#34;HttpHeaders&amp;#34;: { &amp;#34;User-Agent&amp;#34;: &amp;#34;Docker-Client/***&amp;#34; } } 5.2. 创建ImagePullSecret #  执行如下命令创建ImagePullSecret
kubectl create secret generic harbor-admin-secret --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson --namespace hnbcao-mixing-ore 说明：
 harbor-admin-secret： ImagePullSecret名字 type： 指定secret类型为kubernetes.io/dockerconfigjson namespace：secret命名空间  5.3. 添加ImagePullSecret #   Deployment  在配置项的spec.template.spec.imagePullSecrets下添加secret：harbor-admin-secret。例如，Deployment的配置如下：
kind: Deployment apiVersion: apps/v1 metadata: name: app-test spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: app-test app.kubernetes.io/name: hnbcao template: metadata: labels: app.</description>
    </item>
    
    <item>
      <title>6. Kubernetes集群运行问题记录</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/6-kubernetes-seo/</link>
      <pubDate>Thu, 02 Jan 2020 09:17:08 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/6-kubernetes-seo/</guid>
      <description>kubernetes集群运行问题记录 #  6.1. 集群内部容器无法解析外部DNS #    原因：由于节点名为nodex.domain.xx，集群内部容器的/etc/resolv.conf中search域会domain.xx域，容器在解析外网域名时默认会先在外网域名后添加.domain.xx进行域名解析，而外网存在域名domain.xx，且DNS解析域名为*.domain.xx，所以会将外网域名解析至*.domain.xx对应的IP地址，最终导致容器内部无法访问外网域名。
  解决办法：尽量避免集群内部节点名与外网域名冲突，可采用domain.local结尾等命名方式为节点命名。
  6.2. gitlab迁移之后系统异常 #    原因：集群新建gitlab仓库各组件之间的认证文件与原有gialab不一致，导致恢复数据之后部分组件之间交互异常。
  解决办法：新建gitlab之前，迁移原有gitlab中所有的secret文件。
  6.3. kubernetes证书相关问题 #    原因：由于没有配置etcd证书的sans，导致集群master节点故障时，etcd无法启动，集群崩溃。
  解决办法：
 查看etcd日志，发现有出现关于证书的错误信息。master节点上执行openssl x509 -text -in /etc/kubernetes/pki/etcd/server.crt -noout查看证书的sans。输出证书信息为：  Certificate: ... X509v3 extensions: ... X509v3 Subject Alternative Name: DNS:master1.segma.local, DNS:localhost, IP Address:192.168.1.202, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ... 其中X509v3 Subject Alternative Name项中，DNS和IP地址不包括其他主节点地址，所以证书不完整，需要重新生成证书。
以下所有操作在所有主节点上执行。首先备份/etc/kubernetes/pki下所有文件  cp -r /etc/kubernetes/pki /etc/kubernetes/pki_backup 使用kubeadm生成证书，新建kubeadm config文件，填写其他master节点信息。内容如下：</description>
    </item>
    
    <item>
      <title>7. Kubernetes集群GPU共享解决方案</title>
      <link>https://hnbcao.vip/docs/container/kubernetes/base/7-kubernetes-with-aliyungpu/</link>
      <pubDate>Tue, 16 Jun 2020 14:16:38 +0000</pubDate>
      
      <guid>https://hnbcao.vip/docs/container/kubernetes/base/7-kubernetes-with-aliyungpu/</guid>
      <description>Kubernetes集群GPU共享解决方案 #  基于阿里云的GPU共享方案 #  https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/docs/install.md</description>
    </item>
    
  </channel>
</rss>
