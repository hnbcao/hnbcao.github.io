<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kubernetes集群安装</title>
      <link href="/2019/12/31/kubernetes-ji-qun-an-zhuang/"/>
      <url>/2019/12/31/kubernetes-ji-qun-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h2 id="kubernetes-高可用master集群部署（使用kubeadm，离线安装，最新支持kubernetes-v1-15-3）"><a href="#kubernetes-高可用master集群部署（使用kubeadm，离线安装，最新支持kubernetes-v1-15-3）" class="headerlink" title="kubernetes 高可用master集群部署（使用kubeadm，离线安装，最新支持kubernetes v1.15.3）"></a>kubernetes 高可用master集群部署（使用kubeadm，离线安装，最新支持kubernetes v1.15.3）</h2><ul><li>1、本文基于<a href="https://www.kubernetes.org.cn/4948.html" target="_blank" rel="noopener">kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora</a>编写，修改了master之间的负载均衡方式为HAProxy+keeplived方式。</li><li>2、此离线教程必须保证目标安装环境与离线包下载环境一致，或者是考虑做yum镜像源。</li><li>3、关于keepalived+haproxy负载均衡，由于是在阿里云上搭建的，事实上是没有实现的，至于为何也成功部署了环境，其实是每台机器上keepalived都处于激活状态，对虚拟ip的访问都映射到了本机，本机又通过haproxy将请求负载到了api-server上。这是个神奇的事情，直到现在才搞清楚keepalived+haproxy的原理，如果是在阿里云上部署，这块建议使用阿里云的负载均衡功能。（keepalived+haproxy是为了实现api-server的负载均衡）</li><li>4、关于内核，实际上升不升级应该问题都不是很大，至少目前环境没出现过问题。</li><li>5、关于kubernetes版本，目前该教程能支持最新的v1.15.3版本的安装，注意修改版本号。</li></ul><p>集群方案：</p><ul><li>发行版：CentOS 7</li><li>容器运行时</li><li>内核： 4.18.12-1.el7.elrepo.x86_64</li><li>版本：Kubernetes: 1.14.0</li><li>网络方案: Calico</li><li>kube-proxy mode: IPVS</li><li>master高可用方案：HAProxy keepalived LVS</li><li>DNS插件: CoreDNS</li><li>metrics插件：metrics-server</li><li>界面：kubernetes-dashboard</li></ul><h3 id="Kubernetes集群搭建"><a href="#Kubernetes集群搭建" class="headerlink" title="Kubernetes集群搭建"></a>Kubernetes集群搭建</h3><table><thead><tr><th>Host Name</th><th>Role</th><th>IP</th></tr></thead><tbody><tr><td>master1</td><td>master1</td><td>192.168.56.103</td></tr><tr><td>master2</td><td>master2</td><td>192.168.56.104</td></tr><tr><td>master3</td><td>master3</td><td>192.168.56.105</td></tr><tr><td>node1</td><td>node1</td><td>192.168.56.106</td></tr><tr><td>node2</td><td>node2</td><td>192.168.56.107</td></tr><tr><td>node3</td><td>node3</td><td>192.168.56.108</td></tr></tbody></table><p>1、离线安装包准备（基于能够访问外网的服务器下载相应安装包）</p><pre class=" language-sh"><code class="language-sh"># 设置yum缓存路径，cachedir 缓存路径 keepcache=1保持安装包在软件安装之后不删除cat /etc/yum.conf  [main]cachedir=/home/yumkeepcache=1# 安装ifconfigyum install net-tools -y# 时间同步yum install -y ntpdate# 安装docker（建议18.06.3.ce）yum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repoyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum makecache fast## 列出Docker版本yum list docker-ce --showduplicates | sort -r## 安装指定版本sudo yum install docker-ce-<VERSION_STRING>eg:sudo yum install docker-ce-18.06.3.ce# 安装文件管理器，XShell可通过rz sz命令上传或者下载服务器文件yum install lrzsz -y# 安装keepalived、haproxyyum install -y socat keepalived ipvsadm haproxy# 安装kubernetes相关组件cat <<EOF > /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 建议指定各个软件的版本号，使用yum list 软件名（如kubelet） --showduplicates | sort -r列出版本号。yum install -y kubelet kubeadm kubectl ebtables# 其他软件安装yum install wget# 拷贝离线包到集群节点# 安装# rpm -ivh *.rpm --force --nodepsrpm -ivh ./base/packages/*.rpm --nodeps --forcerpm -ivh ./docker-ce-stable/packages/*.rpm --nodeps --forcerpm -ivh ./extras/packages/*.rpm --nodeps --forcerpm -ivh ./kubernetes/packages/*.rpm --nodeps --forcerpm -ivh ./updates/packages/*.rpm --nodeps --force</code></pre><p>2、节点系统配置</p><ul><li>关闭SELinux、防火墙</li></ul><pre class=" language-sh"><code class="language-sh">systemctl stop firewalldsystemctl disable firewalldsetenforce 0sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config</code></pre><ul><li>关闭系统的Swap（Kubernetes 1.8开始要求）</li></ul><pre class=" language-sh"><code class="language-sh">swapoff -ayes | cp /etc/fstab /etc/fstab_bakcat /etc/fstab_bak |grep -v swap > /etc/fstab</code></pre><ul><li>配置L2网桥在转发包时会被iptables的FORWARD规则所过滤，该配置被CNI插件需要，更多信息请参考<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements" target="_blank" rel="noopener">Network Plugin Requirements</a></li></ul><pre class=" language-sh"><code class="language-sh">echo """vm.swappiness = 0net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1""" > /etc/sysctl.confsysctl -p</code></pre><p><a href="https://www.cnblogs.com/zejin2008/p/7102485.html" target="_blank" rel="noopener">centos7添加bridge-nf-call-ip6tables出现No such file or directory</a>,简单来说就是执行一下 modprobe br_netfilter</p><ul><li>同步时间</li></ul><pre class=" language-sh"><code class="language-sh">ntpdate -u ntp.api.bz</code></pre><ul><li>升级内核到最新（已准备内核离线安装包，可选）</li></ul><p><a href="https://www.aliyun.com/jiaocheng/130885.html" target="_blank" rel="noopener">centos7 升级内核</a></p><p><a href="https://www.kubernetes.org.cn/5163.html" target="_blank" rel="noopener">参考文章</a></p><pre class=" language-sh"><code class="language-sh">grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfggrubby --default-kernelgrubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"</code></pre><ul><li>重启系统，确认内核版本后，开启IPVS（如果未升级内核，去掉ip_vs_fo）</li></ul><pre class=" language-sh"><code class="language-sh">uname -acat > /etc/sysconfig/modules/ipvs.modules <<EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack"for kernel_module in \${ipvs_modules}; do /sbin/modinfo -F filename \${kernel_module} > /dev/null 2>&1 if [ $? -eq 0 ]; then /sbin/modprobe \${kernel_module} fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs</code></pre><p>执行sysctl -p报错可执行modprobe br_netfilter，请参考<a href="https://www.cnblogs.com/zejin2008/p/7102485.html" target="_blank" rel="noopener">centos7添加bridge-nf-call-ip6tables出现No such file or directory</a></p><ul><li>所有机器需要设定/etc/sysctl.d/k8s.conf的系统参数(可选)</li></ul><pre class=" language-sh"><code class="language-sh"># https://github.com/moby/moby/issues/31208 # ipvsadm -l --timout# 修复ipvs模式下长连接timeout问题 小于900即可cat <<EOF > /etc/sysctl.d/k8s.confnet.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1net.ipv4.neigh.default.gc_stale_time = 120net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.default.arp_announce = 2net.ipv4.conf.lo.arp_announce = 2net.ipv4.conf.all.arp_announce = 2net.ipv4.ip_forward = 1net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 1024net.ipv4.tcp_synack_retries = 2net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.netfilter.nf_conntrack_max = 2310720fs.inotify.max_user_watches=89100fs.may_detach_mounts = 1fs.file-max = 52706963fs.nr_open = 52706963net.bridge.bridge-nf-call-arptables = 1vm.swappiness = 0vm.overcommit_memory=1vm.panic_on_oom=0EOFsysctl --system</code></pre><ul><li>设置开机启动</li></ul><pre class=" language-sh"><code class="language-sh"># 启动dockersed -i "13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT" /usr/lib/systemd/system/docker.servicesystemctl daemon-reloadsystemctl enable dockersystemctl start docker# 设置kubelet开机启动systemctl enable kubeletsystemctl enable keepalivedsystemctl enable haproxy</code></pre><ul><li>设置免密登录</li></ul><pre class=" language-sh"><code class="language-sh"># 1、三次回车后，密钥生成完成ssh-keygen# 2、拷贝密钥到其他节点ssh-copy-id -i ~/.ssh/id_rsa.pub  用户名字@192.168.x.xxx</code></pre><p>**、 Kubernetes要求集群中所有机器具有不同的Mac地址、产品uuid、Hostname。</p><p>3、keepalived+haproxy配置</p><pre class=" language-sh"><code class="language-sh">cd ~/# 创建集群信息文件echo """CP0_IP=192.168.56.103CP1_IP=192.168.56.103CP2_IP=192.168.56.104VIP=192.168.56.102NET_IF=eth0CIDR=10.244.0.0/16""" > ./cluster-infobash -c "$(curl -fsSL https://raw.githubusercontent.com/hnbcao/kubeadm-ha-master/v1.14.0/keepalived-haproxy.sh)"</code></pre><p>4、部署HA Master</p><p>HA Master的部署过程已经自动化，请在master-1上执行如下命令，并注意修改IP;</p><p>脚本主要执行三步：</p><p>1)、重置kubelet设置</p><pre class=" language-sh"><code class="language-sh">kubeadm reset -frm -rf /etc/kubernetes/pki/</code></pre><p>2)、编写节点配置文件并初始化master1的kubelet</p><pre class=" language-sh"><code class="language-sh">echo """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.14.0controlPlaneEndpoint: "${VIP}:8443"maxPods: 100networkPlugin: cniimageRepository: registry.aliyuncs.com/google_containersapiServer:  certSANs:  - ${CP0_IP}  - ${CP1_IP}  - ${CP2_IP}  - ${VIP}networking:  # This CIDR is a Calico default. Substitute or remove for your CNI provider.  podSubnet: ${CIDR}---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs""" > /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yamlmkdir -p $HOME/.kubecp -f /etc/kubernetes/admin.conf ${HOME}/.kube/config</code></pre><ul><li>关于默认网关问题，如果有多张网卡，需要先将默认网关切换到集群使用的那张网卡上，否则可能会出现etcd无法连接等问题。（应用我用的虚拟机，有一张网卡无法做到各个节点胡同；route查看当前网关信息，route del default删除默认网关，route add default enth0设置默认网关enth0为网卡名）</li></ul><p>3)、拷贝相关证书到master2、master3</p><pre class=" language-sh"><code class="language-sh">for index in 1 2; do  ip=${IPS[${index}]}  ssh $ip "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/"  scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt  scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key  scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key  scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub  scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt  scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key  scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt  scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key  scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf  scp /etc/kubernetes/admin.conf $ip:~/.kube/config  ssh ${ip} "${JOIN_CMD} --experimental-control-plane"done</code></pre><p>4)、master2、master3加入节点</p><pre class=" language-sh"><code class="language-sh">JOIN_CMD=`kubeadm token create --print-join-command`ssh ${ip} "${JOIN_CMD} --experimental-control-plane"</code></pre><p>完整脚本：</p><pre class=" language-sh"><code class="language-sh"># 部署HA masterbash -c "$(curl -fsSL https://raw.githubusercontent.com/hnbcao/kubeadm-ha-master/v1.14.0/kube-ha.sh)"</code></pre><p>5、加入节点（这是个错误的操作，并不需要在node部署keepalived+haproxy，如果node节点无法ping通虚拟IP（VIP），其原因是当前环境无法实现vip，具体原因由于能力有限，只能麻烦自己找找咯，方便分享的话不胜感激。）</p><ul><li>各个节点需要配置keepalived 和 haproxy</li></ul><pre class=" language-sh"><code class="language-sh">#/etc/haproxy/haproxy.cfgglobal    log         127.0.0.1 local2    chroot      /var/lib/haproxy    pidfile     /var/run/haproxy.pid    maxconn     4000    user        haproxy    group       haproxy    daemon    stats socket /var/lib/haproxy/statsdefaults    mode                    tcp    log                     global    option                  tcplog    option                  dontlognull    option                  redispatch    retries                 3    timeout queue           1m    timeout connect         10s    timeout client          1m    timeout server          1m    timeout check           10s    maxconn                 3000listen stats    mode   http    bind :10086    stats   enable    stats   uri     /admin?stats    stats   auth    admin:admin    stats   admin   if TRUEfrontend  k8s_https *:8443    mode      tcp    maxconn      2000    default_backend     https_sribackend https_sri    balance      roundrobin    server master1-api ${MASTER1_IP}:6443  check inter 10000 fall 2 rise 2 weight 1    server master2-api ${MASTER2_IP}:6443  check inter 10000 fall 2 rise 2 weight 1    server master3-api ${MASTER3_IP}:6443  check inter 10000 fall 2 rise 2 weight 1</code></pre><pre class=" language-sh"><code class="language-sh">#/etc/keepalived/keepalived.conf global_defs {    router_id LVS_DEVEL}vrrp_script check_haproxy {    script /etc/keepalived/check_haproxy.sh    interval 3}vrrp_instance VI_1 {    state MASTER    interface eth0    virtual_router_id 80    priority 100    advert_int 1    authentication {        auth_type PASS        auth_pass just0kk    }    virtual_ipaddress {        ${VIP}/24    }    track_script {           check_haproxy    }}</code></pre><pre class=" language-sh"><code class="language-sh">/etc/keepalived/check_haproxy.sh#!/bin/bashA=`ps -C haproxy --no-header |wc -l`if [ $A -eq 0 ];then/etc/init.d/keepalived stopfi</code></pre><p>注意两个配置中的${MASTER1 _ IP}, ${MASTER2 _ IP}, ${MASTER3 _ IP}、${VIP}需要替换为自己集群相应的IP地址</p><ul><li>重启keepalived和haproxy</li></ul><pre class=" language-sh"><code class="language-sh">systemctl stop keepalivedsystemctl enable keepalivedsystemctl start keepalivedsystemctl stop haproxysystemctl enable haproxysystemctl start haproxy</code></pre><ul><li>节点加入命令获取</li></ul><pre class=" language-sh"><code class="language-sh">#master节点执行该命令，再在节点执行获取到的命令kubeadm token create --print-join-command</code></pre><p>6、结束安装</p><p>此时集群还需要安装网络组件，我选择了calico。具体安装方式可访问<a href="https://www.projectcalico.org/" target="_blank" rel="noopener">calico官网</a>，或者运行本仓库里面addons/calico下的配置。注意替换里面的镜像和Deployment里面的环境变量CALICO_IPV4POOL_CIDR为/etc/kubernetes/kubeadm-config.yaml里面networking.podSubnet的值。</p><p>文章只是在文章<a href="https://www.kubernetes.org.cn/4948.html" target="_blank" rel="noopener">kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora</a>的基础上，修改了master的HA方案。关于集群安装的详细步骤，建议访问<a href="https://www.kubernetes.org.cn/4948.html" target="_blank" rel="noopener">kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora</a>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> 运维 </tag>
            
            <tag> 容器化 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
