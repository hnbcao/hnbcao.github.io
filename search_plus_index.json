{"./":{"url":"./","title":"目录","keywords":"","body":"目录 Go 语言学习笔记 基础知识 常用关键字 for 和 range 常用关键字 select 运行时 上下文 Context 同步原语与锁 进阶内容 逃逸分析 Java 语言学习笔记 Java容器学习笔记 Spring-Kafka参数配置详情 容器化 Kubernetes Kubernetes集群安装 Kubernetes集群安装Cert-Manager Kubernetes集群安装Traefik-Ingress Kubernetes集群创建用户 Kubernetes集群创建Image-Pull-Secret Kubernetes集群跨namespace服务访问 kubernetes集群运行问题记录 Kubernetes集群GPU共享解决方案 Docker DockerCompose安装使用 运维管理 CentOS离线镜像仓库创建 Ceph集群安装 Fluent-bit日志插件配置说明 FTP服务器搭建 软件工程 设计模式简介 "},"golang/foundation/":{"url":"golang/foundation/","title":"基础知识","keywords":"","body":"基础知识 "},"golang/foundation/常用关键字-for和range.html":{"url":"golang/foundation/常用关键字-for和range.html","title":"常用关键字 for 和 range","keywords":"","body":"for 和 range 数组和切片 循环永动机：对于所有的 range 循环，Go 语言都会在编译期将原切片或者数组赋值给一个新变量 ha，在赋值的过程中就发生了拷贝，而我们又通过 len 关键字预先获取了切片的长度，所以在循环中追加新的元素也不会改变循环执行的次数，这也就解释了循环永动机一节提到的现象。 哈希表 首先会选出一个绿色的正常桶开始遍历，随后遍历所有黄色的溢出桶，最后依次按照索引顺序遍历哈希表中其他的桶，直到所有的桶都被遍历完成。 字符串 遍历字符串时拿到的值都是 rune 类型的变量，for i, r := range s {} 的结构都会被转换成如下所示的形式： ha := s for hv1 := 0; hv1 通道 使用 range 遍历 Channel 也是比较常见的做法，一个形如 for v := range ch {} 的语句最终会被转换成如下的格式： ha := a hv1, hb := ​ 该循环会使用 从管道中取出等待处理的值，这个操作会调用 runtime.chanrecv2 并阻塞当前的协程，当 runtime.chanrecv2 返回时会根据布尔值 hb 判断当前的值是否存在： 如果不存在当前值，意味着当前的管道已经被关闭； 如果存在当前值，会为 v1 赋值并清除 hv1 变量中的数据，然后重新陷入阻塞等待新数据； "},"golang/foundation/常用关键字-select.html":{"url":"golang/foundation/常用关键字-select.html","title":"常用关键字 select","keywords":"","body":"Select 关键字 实现原理 直接阻塞：空 select 语句；空的 select 语句会直接阻塞当前的 Goroutine，导致 Goroutine 进入无法被唤醒的永久休眠状态。 单一管道：select 条件只包含一个 case；如果当前的 select 条件只包含一个 case，当 case 中的 Channel 是空指针时，就会直接挂起当前 Goroutine 并永久休眠。 非阻塞操作：当 select 中仅包含两个 case，并且其中一个是 default 时，Go 语言的编译器就会认为这是一次非阻塞的收发操作。如果 select 控制结构中包含 default 语句，那么这个 select 语句在执行时会遇到以下两种情况： 1.当存在可以收发的 Channel 时，直接处理该 Channel 对应的 case； 2.当不存在可以收发的 Channel 是，执行 default 中的语句； 当我们运行下面的代码时就不会阻塞当前的 Goroutine，它会直接执行 default 中的代码并返回。 func main() { ch := make(chan int) select { case i := 小结 我们简单总结一下 select 结构的执行过程与实现原理，首先在编译期间，Go 语言会对 select 语句进行优化，它会根据 select 中 case 的不同选择不同的优化路径： 空的 select 语句会被转换成调用 runtime.block 直接挂起当前 Goroutine； 如果 select 语句中只包含一个 case，编译器会将其转换成 if ch == nil { block }; n; 表达式； 首先判断操作的 Channel 是不是空的； 然后执行 case 结构中的内容； 如果 select 语句中只包含两个 case 并且其中一个是 default，那么会使用 runtime.selectnbrecv 和 runtime.selectnbsend 非阻塞地执行收发操作； 在默认情况下会通过 runtime.selectgo 获取执行 case 的索引，并通过多个 if 语句执行对应 case 中的代码； 在编译器已经对 select 语句进行优化之后，Go 语言会在运行时执行编译期间展开的 runtime.selectgo 函数，该函数会按照以下的流程执行： 随机生成一个遍历的轮询顺序 pollOrder 并根据 Channel 地址生成锁定顺序 lockOrder； 根据 pollOrder 遍历所有的 case 查看是否有可以立刻处理的 Channel； 如果存在，直接获取 case 对应的索引并返回； 如果不存在，创建 runtime.sudog 结构体，将当前 Goroutine 加入到所有相关 Channel 的收发队列，并调用 runtime.gopark 挂起当前 Goroutine 等待调度器的唤醒； 当调度器唤醒当前 Goroutine 时，会再次按照 lockOrder 遍历所有的 case，从中查找需要被处理的 runtime.sudog 对应的索引； select 关键字是 Go 语言特有的控制结构，它的实现原理比较复杂，需要编译器和运行时函数的通力合作。 "},"golang/runtime/":{"url":"golang/runtime/","title":"运行时","keywords":"","body":"运行时 "},"golang/runtime/context.html":{"url":"golang/runtime/context.html","title":"上下文 Context","keywords":"","body":"上下文 Context 方法实现 context.Context 是 Go 语言在 1.7 版本中引入标准库的接口1，该接口定义了四个需要实现的方法，其中包括： Deadline — 返回 context.Context 被取消的时间，也就是完成工作的截止日期； Done — 返回一个 Channel，这个 Channel 会在当前工作完成或者上下文被取消后关闭，多次调用 Done 方法会返回同一个 Channel； Err — 返回 context.Context 结束的原因，它只会在 Done 方法对应的 Channel 关闭时返回非空的值； 如果 context.Context 被取消，会返回 Canceled 错误； 如果 context.Context 超时，会返回 DeadlineExceeded 错误； Value — 从 context.Context 中获取键对应的值，对于同一个上下文来说，多次调用 Value 并传入相同的 Key 会返回相同的结果，该方法可以用来传递请求特定的数据； 默认上下文 从源代码来看，context.Background 和 context.TODO 也只是互为别名，没有太大的差别，只是在使用和语义上稍有不同： context.Background 是上下文的默认值，所有其他的上下文都应该从它衍生出来； context.TODO 应该仅在不确定应该使用哪种上下文时使用； 在多数情况下，如果当前函数没有上下文作为入参，我们都会使用 context.Background 作为起始的上下文向下传递。 小结 Go 语言中的 context.Context 的主要作用还是在多个 Goroutine 组成的树中同步取消信号以减少对资源的消耗和占用，虽然它也有传值的功能，但是这个功能我们还是很少用到。 在真正使用传值的功能时我们也应该非常谨慎，使用 context.Context 进行传递参数请求的所有参数一种非常差的设计，比较常见的使用场景是传递请求对应用户的认证令牌以及用于进行分布式追踪的请求 ID。 "},"golang/runtime/mutex.html":{"url":"golang/runtime/mutex.html","title":"同步原语与锁","keywords":"","body":"同步原语与锁 概述 ​ 本节会介绍 Go 语言中常见的同步原语 sync.Mutex、sync.RWMutex、sync.WaitGroup、sync.Once 和 sync.Cond 以及扩展原语 golang/sync/errgroup.Group、golang/sync/semaphore.Weighted 和 golang/sync/singleflight.Group 的实现原理，同时也会涉及互斥锁、信号量等并发编程中的常见概念。 互斥锁（Mutex） 正常模式和饥饿模式 sync.Mutex 有两种模式 — 正常模式和饥饿模式。我们需要在这里先了解正常模式和饥饿模式都是什么以及它们有什么样的关系。 在正常模式下，锁的等待者会按照先进先出的顺序获取锁。但是刚被唤起的 Goroutine 与新创建的 Goroutine 竞争时，大概率会获取不到锁，为了减少这种情况的出现，一旦 Goroutine 超过 1ms 没有获取到锁，它就会将当前互斥锁切换饥饿模式，防止部分 Goroutine 被『饿死』。 在饥饿模式中，互斥锁会直接交给等待队列最前面的 Goroutine。新的 Goroutine 在该状态下不能获取锁、也不会进入自旋状态，它们只会在队列的末尾等待。如果一个 Goroutine 获得了互斥锁并且它在队列的末尾或者它等待的时间少于 1ms，那么当前的互斥锁就会切换回正常模式。 加锁和解锁 如果互斥锁的状态不是 0 时就会调用 sync.Mutex.lockSlow 尝试通过自旋（Spinnig）等方式等待锁的释放，该方法的主体是一个非常大 for 循环，这里将它分成几个部分介绍获取锁的过程： 判断当前 Goroutine 能否进入自旋； 通过自旋等待互斥锁的释放； 计算互斥锁的最新状态； 更新互斥锁的状态并获取锁； 自旋的条件 Goroutine 进入自旋的条件非常苛刻： 互斥锁只有在普通模式才能进入自旋； runtime.sync_runtime_canSpin需要返回true： 运行在多 CPU 的机器上； 当前 Goroutine 为了获取该锁进入自旋的次数小于四次； 当前机器上至少存在一个正在运行的处理器 P 并且处理的运行队列为空； 一旦当前 Goroutine 能够进入自旋就会调用runtime.sync_runtime_doSpin和 runtime.procyield并执行 30 次的 PAUSE 指令，该指令只会占用 CPU 并消耗 CPU 时间： 小结 我们已经从多个方面分析了互斥锁 sync.Mutex 的实现原理，这里我们从加锁和解锁两个方面总结注意事项。 互斥锁的加锁过程比较复杂，它涉及自旋、信号量以及调度等概念： 如果互斥锁处于初始化状态，会通过置位 mutexLocked 加锁； 如果互斥锁处于 mutexLocked 状态并且在普通模式下工作，会进入自旋，执行 30 次 PAUSE 指令消耗 CPU 时间等待锁的释放； 如果当前 Goroutine 等待锁的时间超过了 1ms，互斥锁就会切换到饥饿模式； 互斥锁在正常情况下会通过 runtime.sync_runtime_SemacquireMutex将尝试获取锁的 Goroutine 切换至休眠状态，等待锁的持有者唤醒； 如果当前 Goroutine 是互斥锁上的最后一个等待的协程或者等待的时间小于 1ms，那么它会将互斥锁切换回正常模式； 互斥锁的解锁过程与之相比就比较简单，其代码行数不多、逻辑清晰，也比较容易理解： 当互斥锁已经被解锁时，调用 sync.Mutex.Unlock会直接抛出异常； 当互斥锁处于饥饿模式时，将锁的所有权交给队列中的下一个等待者，等待者会负责设置 mutexLocked 标志位； 当互斥锁处于普通模式时，如果没有 Goroutine 等待锁的释放或者已经有被唤醒的 Goroutine 获得了锁，会直接返回；在其他情况下会通过 sync.runtime_Semrelease唤醒对应的 Goroutine； 读写锁（RWMutex） "},"golang/advanced/":{"url":"golang/advanced/","title":"进阶内容","keywords":"","body":"进阶内容 "},"golang/advanced/escape_analysis.html":{"url":"golang/advanced/escape_analysis.html","title":"逃逸分析","keywords":"","body":"逃逸分析 什么是逃逸分析？ 逃逸分析（Escape analysis）: 由语言决定变量分配到堆上还是栈上。在Java中，逃逸分析是在运行时发生；在Go语言中，逃逸分析在编译期间完成，编译器决定内存分配的位置，不需要程序员指定。 在函数中申请一个新的对象： 如果分配在栈中，则函数执行结束可自动将内存回收； 如果分配在堆中，则函数执行结束可交给GC（垃圾回收）处理; 逃逸分析针对指针和大对象：除大对象外，一个值引用变量如果没有被取址，那么它永远不可能逃逸。 逃逸场景 指针逃逸、动态类型逃逸、闭包引用对象逃逸属于指针逃逸，都会发生指针的传递；栈空间不足逃逸属于大对象逃逸，不一定有值传递，这种场景是由于对象过大，无法在栈上分配导致。 指针逃逸： Go可以返回局部变量指针，示例代码如下： package main type User struct { Name string } func main() { user := structFunc() user.Name = \"2\" } func structFunc() *User { // 局部变量user逃逸到堆 user := &User{ Name: \"123\", } user.Name = \"234\" return user } user 本身为一指针，其值通过函数返回值返回，其指向的内存地址不会是栈而是堆，这就是典型的逃逸案例，如果返回值不是指针而是值，此时会发生值拷贝，不会出现逃逸分析。 动态类型逃逸（不确定长度大小）:很多函数参数为interface类型，比如fmt.Println(a …interface{})，编译期间很难确定其参数的具体类型，也能产生逃逸。 如下代码所示： package main import \"fmt\" func main() { s := \"Escape\" fmt.Println(s) } 闭包引用对象逃逸: package main import \"fmt\" func Fibonacci() func() int { a, b := 0, 1 return func() int { a, b = b, a+b return a } } func main() { f := Fibonacci() for i := 0; i Fibonacci()函数中原本属于局部变量的a和b由于闭包的引用，不得不将二者放到堆上，以致产生逃逸。 栈空间不足逃逸（空间开辟过大）： package main func Slice() { s := make([]int, 10000, 10000) for index, _ := range s { s[index] = index } } func main() { Slice() } 当切片长度扩大到10000时就会逃逸。 实际上当栈空间不足以存放当前对象时或无法判断当前切片长度时会将对象分配到堆中。 逃逸分析的作用 逃逸分析的好处是为了减少gc的压力，不逃逸的对象分配在栈上，当函数返回时就回收了资源，不需要gc标记清除。 逃逸分析完后可以确定哪些变量可以分配在栈上，栈的分配比堆快，性能好(逃逸的局部变量会在堆上分配 ,而没有发生逃逸的则有编译器在栈上分配)。 同步消除，如果你定义的对象的方法上有同步锁，但在运行时，却只有一个线程在访问，此时逃逸分析后的机器码，会去掉同步锁运行。 逃逸总结 栈上分配内存比在堆中分配内存有更高的效率 栈上分配的内存不需要GC处理 堆上分配的内存使用完毕会交给GC处理 逃逸分析目的是决定内分配地址是栈还是堆 逃逸分析在编译阶段完成 "},"java/Java容器学习笔记.html":{"url":"java/Java容器学习笔记.html","title":"Java容器学习笔记","keywords":"","body":"Java容器学习笔记 ArrayList与LinkList对比： 性能：《Java编程思想》指出，ArrayList插入移除元素较慢，ArrayList添加元素的速度比LinkList快。。LinkList插入移除元素速度优于ArrayList，且在数据量大时，ArrayList移除元素异常缓慢（按照元素顺序插入移除，若倒序插入移除则快于LinkList），其中的原因是LinkList使用双向链表存储数据，移除时只需要修改待移除元素的前后节点的next与prev位置即可，而ArrayList则涉及到数组的拷贝，倒序的情况下，ArrayList只需要将末尾元素移除即可。 建议：在涉及元素删除的List中，建议使用LinkList，其他情况可使用ArrayList。ArrayList使用在查询比较多，但是插入和删除比较少的情况，而LinkedList用在查询比较少而插入删除比较多的情况。 ArrayList、LinkList、Vector、Stack中只有Stack、Vector是线程安全的类，线程安全的List还有CopyOnWriteArrayList和Collections.synchronizedList()。Collections.synchronizedList()使用装饰模式为传入的List操作加上同步锁。 Stack底层数据结构是Vector，Vector的所有操作都加了synchronized关键字，Vector的底层使用数组保存数据，类似与ArrayList。一般多线程状态下使用List会选择Vector。 HashSet底层数据结构是HashMap LinkedHashSet底层数据结构是LinkedHashMap "},"java/Spring-Kafka参数配置详情.html":{"url":"java/Spring-Kafka参数配置详情.html","title":"Spring-Kafka参数配置详情","keywords":"","body":"Spring Kafka参数配置详情 一、全局配置 # 用逗号分隔的主机:端口对列表，用于建立到Kafka群集的初始连接。覆盖全局连接设置属性 spring.kafka.bootstrap-servers # 在发出请求时传递给服务器的ID。用于服务器端日志记录 spring.kafka.client-id，默认无 # 用于配置客户端的其他属性，生产者和消费者共有的属性 spring.kafka.properties.* # 消息发送的默认主题，默认无 spring.kafka.template.default-topic 二、生产者 Spring Boot中，Kafka 生产者相关配置(所有配置前缀为spring.kafka.producer.)： # 生产者要求Leader在考虑请求完成之前收到的确认数 spring.kafka.producer.acks # 默认批量大小。较小的批处理大小将使批处理不太常见，并可能降低吞吐量（批处理大小为零将完全禁用批处理） spring.kafka.producer.batch-size spring.kafka.producer.bootstrap-servers # 生产者可用于缓冲等待发送到服务器的记录的总内存大小。 spring.kafka.producer.buffer-memory # 在发出请求时传递给服务器的ID。用于服务器端日志记录。 spring.kafka.producer.client-id # 生产者生成的所有数据的压缩类型 spring.kafka.producer.compression-type # 键的序列化程序类 spring.kafka.producer.key-serializer spring.kafka.producer.properties.* # 大于零时，启用失败发送的重试次数 spring.kafka.producer.retries spring.kafka.producer.ssl.key-password spring.kafka.producer.ssl.key-store-location spring.kafka.producer.ssl.key-store-password spring.kafka.producer.ssl.key-store-type spring.kafka.producer.ssl.protocol spring.kafka.producer.ssl.trust-store-location spring.kafka.producer.ssl.trust-store-password spring.kafka.producer.ssl.trust-store-type # 非空时，启用对生产者的事务支持 spring.kafka.producer.transaction-id-prefix spring.kafka.producer.value-serializer 三、消费者 Spring Boot中，Kafka 消费者相关配置(所有配置前缀为spring.kafka.consumer.)： # 如果“enable.auto.commit”设置为true，设置消费者偏移自动提交到Kafka的频率，默认值无，单位毫秒(ms) spring.kafka.consumer.auto-commit-interval # 当Kafka中没有初始偏移或服务器上不再存在当前偏移时策略设置，默认值无，latest/earliest/none三个值设置 # earliest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费 # latest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 # none topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常 spring.kafka.consumer.auto-offset-reset # 用逗号分隔的主机:端口对列表，用于建立到Kafka群集的初始连接。覆盖全局连接设置属性 spring.kafka.consumer.bootstrap-servers # 在发出请求时传递给服务器的ID，用于服务器端日志记录 spring.kafka.consumer.client-id # 消费者的偏移量是否在后台定期提交 spring.kafka.consumer.enable-auto-commit # 如果没有足够的数据来立即满足“fetch-min-size”的要求，则服务器在取回请求之前阻塞的最大时间量 spring.kafka.consumer.fetch-max-wait # 服务器应为获取请求返回的最小数据量。 spring.kafka.consumer.fetch-min-size # 标识此消费者所属的默认消费者组的唯一字符串 spring.kafka.consumer.group-id # 消费者协调员的预期心跳间隔时间。 spring.kafka.consumer.heartbeat-interval # 用于读取以事务方式写入的消息的隔离级别。 spring.kafka.consumer.isolation-level # 密钥的反序列化程序类 spring.kafka.consumer.key-deserializer # 在对poll()的单个调用中返回的最大记录数。 spring.kafka.consumer.max-poll-records # 用于配置客户端的其他特定于消费者的属性。 spring.kafka.consumer.properties.* # 密钥存储文件中私钥的密码。 spring.kafka.consumer.ssl.key-password # 密钥存储文件的位置。 spring.kafka.consumer.ssl.key-store-location # 密钥存储文件的存储密码。 spring.kafka.consumer.ssl.key-store-password # 密钥存储的类型，如JKS spring.kafka.consumer.ssl.key-store-type # 要使用的SSL协议，如TLSv1.2, TLSv1.1, TLSv1 spring.kafka.consumer.ssl.protocol # 信任存储文件的位置。 spring.kafka.consumer.ssl.trust-store-location # 信任存储文件的存储密码。 spring.kafka.consumer.ssl.trust-store-password # 信任存储区的类型。 spring.kafka.consumer.ssl.trust-store-type # 值的反序列化程序类。 spring.kafka.consumer.value-deserializer 四、监听器 Spring Boot中，Kafka Listener相关配置(所有配置前缀为spring.kafka.listener.)： # ackMode为“COUNT”或“COUNT_TIME”时偏移提交之间的记录数 spring.kafka.listener.ack-count= spring.kafka.listener.ack-mode spring.kafka.listener.ack-time spring.kafka.listener.client-id spring.kafka.listener.concurrency spring.kafka.listener.idle-event-interval spring.kafka.listener.log-container-config # 如果Broker上不存在至少一个配置的主题（topic），则容器是否无法启动， # 该设置项结合Broker设置项allow.auto.create.topics=true，如果为false，则会自动创建不存在的topic spring.kafka.listener.missing-topics-fatal=true # 非响应消费者的检查间隔时间。如果未指定持续时间后缀，则将使用秒作为单位 spring.kafka.listener.monitor-interval spring.kafka.listener.no-poll-threshold spring.kafka.listener.poll-timeout spring.kafka.listener.type 五、管理 spring.kafka.admin.client-id # 如果启动时代理不可用，是否快速失败 spring.kafka.admin.fail-fast=false spring.kafka.admin.properties.* spring.kafka.admin.ssl.key-password spring.kafka.admin.ssl.key-store-location spring.kafka.admin.ssl.key-store-password spring.kafka.admin.ssl.key-store-type spring.kafka.admin.ssl.protocol spring.kafka.admin.ssl.trust-store-location spring.kafka.admin.ssl.trust-store-password spring.kafka.admin.ssl.trust-store-type 六、授权服务(JAAS) spring.kafka.jaas.control-flag=required spring.kafka.jaas.enabled=false spring.kafka.jaas.login-module=com.sun.security.auth.module.Krb5LoginModule spring.kafka.jaas.options.* 七、SSL认证 spring.kafka.ssl.key-password spring.kafka.ssl.key-store-location spring.kafka.ssl.key-store-password spring.kafka.ssl.key-store-type spring.kafka.ssl.protocol spring.kafka.ssl.trust-store-location spring.kafka.ssl.trust-store-password spring.kafka.ssl.trust-store-type 八、Stream流处理 spring.kafka.streams.application-id spring.kafka.streams.auto-startup spring.kafka.streams.bootstrap-servers spring.kafka.streams.cache-max-size-buffering spring.kafka.streams.client-id spring.kafka.streams.properties.* spring.kafka.streams.replication-factor spring.kafka.streams.ssl.key-password spring.kafka.streams.ssl.key-store-location spring.kafka.streams.ssl.key-store-password spring.kafka.streams.ssl.key-store-type spring.kafka.streams.ssl.protocol spring.kafka.streams.ssl.trust-store-location spring.kafka.streams.ssl.trust-store-password spring.kafka.streams.ssl.trust-store-type spring.kafka.streams.state-dir "},"container/kubernetes/":{"url":"container/kubernetes/","title":"Kubernetes","keywords":"","body":"Kubernetes "},"container/kubernetes/Kubernetes集群安装.html":{"url":"container/kubernetes/Kubernetes集群安装.html","title":"Kubernetes集群安装","keywords":"","body":"Kubernetes集群搭建 1、本文基于kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora编写，修改了master之间的负载均衡方式为HAProxy+keeplived方式。 2、此离线教程必须保证目标安装环境与离线包下载环境一致，或者是考虑做yum镜像源。 3、关于keepalived+haproxy负载均衡，由于是在阿里云上搭建的，事实上是没有实现的，至于为何也成功部署了环境，其实是每台机器上keepalived都处于激活状态，对虚拟ip的访问都映射到了本机，本机又通过haproxy将请求负载到了api-server上。这是个神奇的事情，直到现在才搞清楚keepalived+haproxy的原理，如果是在阿里云上部署，这块建议使用阿里云的负载均衡功能。（keepalived+haproxy是为了实现api-server的负载均衡） 4、关于内核，实际上升不升级应该问题都不是很大，至少目前环境没出现过问题。 5、关于kubernetes版本，目前该教程能支持最新的v1.15.3版本的安装，注意修改版本号。 集群方案： 发行版：CentOS 7 容器运行时 内核： 4.18.12-1.el7.elrepo.x86_64 版本：Kubernetes: 1.14.0 网络方案: Calico kube-proxy mode: IPVS master高可用方案：HAProxy keepalived LVS DNS插件: CoreDNS metrics插件：metrics-server 界面：kubernetes-dashboard 一、环境概述 Host Name Role IP master1 master1 192.168.56.103 master2 master2 192.168.56.104 master3 master3 192.168.56.105 node1 node1 192.168.56.106 node2 node2 192.168.56.107 node3 node3 192.168.56.108 二、离线仓库制作（可选） 具体制作方式见：CentOS离线镜像仓库创建 需要制作的离线仓库有： base repo wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo docker repo [docker-ce-stable] name=Docker CE Stable - $basearch baseurl=https://download.docker.com/linux/centos/$releasever/$basearch/stable enabled=1 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-stable-debuginfo] name=Docker CE Stable - Debuginfo $basearch baseurl=https://download.docker.com/linux/centos/$releasever/debug-$basearch/stable enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-stable-source] name=Docker CE Stable - Sources baseurl=https://download.docker.com/linux/centos/$releasever/source/stable enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-test] name=Docker CE Test - $basearch baseurl=https://download.docker.com/linux/centos/$releasever/$basearch/test enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-test-debuginfo] name=Docker CE Test - Debuginfo $basearch baseurl=https://download.docker.com/linux/centos/$releasever/debug-$basearch/test enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-test-source] name=Docker CE Test - Sources baseurl=https://download.docker.com/linux/centos/$releasever/source/test enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-nightly] name=Docker CE Nightly - $basearch baseurl=https://download.docker.com/linux/centos/$releasever/$basearch/nightly enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-nightly-debuginfo] name=Docker CE Nightly - Debuginfo $basearch baseurl=https://download.docker.com/linux/centos/$releasever/debug-$basearch/nightly enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg [docker-ce-nightly-source] name=Docker CE Nightly - Sources baseurl=https://download.docker.com/linux/centos/$releasever/source/nightly enabled=0 gpgcheck=1 gpgkey=https://download.docker.com/linux/centos/gpg kubernetes repo cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 三、软件安装 # 安装ifconfig yum install net-tools -y # 时间同步 yum install -y ntpdate # 安装docker（建议18.06.3.ce） ## 列出Docker版本 yum list docker-ce --showduplicates | sort -r ## 安装指定版本 sudo yum install docker-ce- eg:sudo yum install docker-ce-18.06.3.ce # 安装文件管理器，XShell可通过rz sz命令上传或者下载服务器文件 yum install lrzsz -y # 安装keepalived、haproxy yum install -y socat keepalived ipvsadm haproxy # 安装kubernetes相关组件 # 建议指定各个软件的版本号，使用yum list 软件名（如kubelet） --showduplicates | sort -r列出版本号。 yum install -y kubelet kubeadm kubectl ebtables # 其他软件安装 yum install wget 四、节点系统配置 关闭SELinux、防火墙 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 关闭系统的Swap（Kubernetes 1.8开始要求） swapoff -a yes | cp /etc/fstab /etc/fstab_bak cat /etc/fstab_bak |grep -v swap > /etc/fstab 配置L2网桥在转发包时会被iptables的FORWARD规则所过滤，该配置被CNI插件需要，更多信息请参考Network Plugin Requirements echo \"\"\" vm.swappiness = 0 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 \"\"\" > /etc/sysctl.conf sysctl -p centos7添加bridge-nf-call-ip6tables出现No such file or directory,简单来说就是执行一下 modprobe br_netfilter 同步时间 ntpdate -u ntp.api.bz 升级内核到最新（已准备内核离线安装包，可选） centos7 升级内核 参考文章 grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg grubby --default-kernel grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" 重启系统，确认内核版本后，开启IPVS（如果未升级内核，去掉ip_vs_fo） uname -a cat > /etc/sysconfig/modules/ipvs.modules /dev/null 2>&1 if [ $? -eq 0 ]; then /sbin/modprobe \\${kernel_module} fi done EOF chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs 执行sysctl -p报错可执行modprobe br_netfilter，请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory 所有机器需要设定/etc/sysctl.d/k8s.conf的系统参数(可选) # https://github.com/moby/moby/issues/31208 # ipvsadm -l --timout # 修复ipvs模式下长连接timeout问题 小于900即可 cat /etc/sysctl.d/k8s.conf net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_probes = 10 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1 net.ipv4.neigh.default.gc_stale_time = 120 net.ipv4.conf.all.rp_filter = 0 net.ipv4.conf.default.rp_filter = 0 net.ipv4.conf.default.arp_announce = 2 net.ipv4.conf.lo.arp_announce = 2 net.ipv4.conf.all.arp_announce = 2 net.ipv4.ip_forward = 1 net.ipv4.tcp_max_tw_buckets = 5000 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 1024 net.ipv4.tcp_synack_retries = 2 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.netfilter.nf_conntrack_max = 2310720 fs.inotify.max_user_watches=89100 fs.may_detach_mounts = 1 fs.file-max = 52706963 fs.nr_open = 52706963 net.bridge.bridge-nf-call-arptables = 1 vm.swappiness = 0 vm.overcommit_memory=1 vm.panic_on_oom=0 EOF sysctl --system 设置开机启动 # 启动docker sed -i \"13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT\" /usr/lib/systemd/system/docker.service systemctl daemon-reload systemctl enable docker systemctl start docker # 设置kubelet开机启动 systemctl enable kubelet systemctl enable keepalived systemctl enable haproxy 设置免密登录 # 1、三次回车后，密钥生成完成 ssh-keygen # 2、拷贝密钥到其他节点 ssh-copy-id -i ~/.ssh/id_rsa.pub 用户名字@192.168.x.xxx **、 Kubernetes要求集群中所有机器具有不同的Mac地址、产品uuid、Hostname。 五、keepalived+haproxy配置 cd ~/ # 创建集群信息文件 echo \"\"\" CP0_IP=192.168.56.103 CP1_IP=192.168.56.103 CP2_IP=192.168.56.104 VIP=192.168.56.102 NET_IF=eth0 CIDR=10.244.0.0/16 \"\"\" > ./cluster-info bash -c \"$(curl -fsSL https://raw.githubusercontent.com/hnbcao/kubeadm-ha-master/v1.14.0/keepalived-haproxy.sh)\" 安装Keepalived、Haproxy 这是个错误的操作，并不需要在node部署keepalived+haproxy，如果node节点无法ping通虚拟IP（VIP），其原因是当前环境无法实现vip，具体原因由于能力有限，只能麻烦自己找找咯，方便分享的话不胜感激。 各个节点需要配置keepalived 和 haproxy #/etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults mode tcp log global option tcplog option dontlognull option redispatch retries 3 timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s maxconn 3000 listen stats mode http bind :10086 stats enable stats uri /admin?stats stats auth admin:admin stats admin if TRUE frontend k8s_https *:8443 mode tcp maxconn 2000 default_backend https_sri backend https_sri balance roundrobin server master1-api ${MASTER1_IP}:6443 check inter 10000 fall 2 rise 2 weight 1 server master2-api ${MASTER2_IP}:6443 check inter 10000 fall 2 rise 2 weight 1 server master3-api ${MASTER3_IP}:6443 check inter 10000 fall 2 rise 2 weight 1 #/etc/keepalived/keepalived.conf global_defs { router_id LVS_DEVEL } vrrp_script check_haproxy { script /etc/keepalived/check_haproxy.sh interval 3 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 80 priority 100 advert_int 1 authentication { auth_type PASS auth_pass just0kk } virtual_ipaddress { ${VIP}/24 } track_script { check_haproxy } } /etc/keepalived/check_haproxy.sh #!/bin/bash A=`ps -C haproxy --no-header |wc -l` if [ $A -eq 0 ];then /etc/init.d/keepalived stop fi 注意两个配置中的${MASTER1 IP}, ${MASTER2 IP}, ${MASTER3 _ IP}、${VIP}需要替换为自己集群相应的IP地址 重启keepalived和haproxy systemctl stop keepalived systemctl enable keepalived systemctl start keepalived systemctl stop haproxy systemctl enable haproxy systemctl start haproxy 六、部署HA Master HA Master的部署过程已经自动化，请在master-1上执行如下命令，并注意修改IP; 脚本主要执行三步： 1)、重置kubelet设置 kubeadm reset -f rm -rf /etc/kubernetes/pki/ 2)、编写节点配置文件并初始化master1的kubelet echo \"\"\" apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: v1.14.0 controlPlaneEndpoint: \"${VIP}:8443\" maxPods: 100 networkPlugin: cni imageRepository: registry.aliyuncs.com/google_containers apiServer: certSANs: - ${CP0_IP} - ${CP1_IP} - ${CP2_IP} - ${VIP} networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: ${CIDR} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs \"\"\" > /etc/kubernetes/kubeadm-config.yaml kubeadm init --config /etc/kubernetes/kubeadm-config.yaml mkdir -p $HOME/.kube cp -f /etc/kubernetes/admin.conf ${HOME}/.kube/config 关于默认网关问题，如果有多张网卡，需要先将默认网关切换到集群使用的那张网卡上，否则可能会出现etcd无法连接等问题。（应用我用的虚拟机，有一张网卡无法做到各个节点胡同；route查看当前网关信息，route del default删除默认网关，route add default enth0设置默认网关enth0为网卡名） 3)、拷贝相关证书到master2、master3 for index in 1 2; do ip=${IPS[${index}]} ssh $ip \"mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/\" scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $ip:~/.kube/config ssh ${ip} \"${JOIN_CMD} --control-plane\" done 4)、master2、master3加入节点 JOIN_CMD=`kubeadm token create --print-join-command` ssh ${ip} \"${JOIN_CMD} --control-plane\" 完整脚本： # 部署HA master bash -c \"$(curl -fsSL https://raw.githubusercontent.com/hnbcao/kubeadm-ha-master/v1.14.0/kube-ha.sh)\" 七、加入节点 节点加入命令获取 #master节点执行该命令，再在节点执行获取到的命令 kubeadm token create --print-join-command 八、结束安装 此时集群还需要安装网络组件，我选择了calico。具体安装方式可访问calico官网，或者运行本仓库里面addons/calico下的配置。注意替换里面的镜像和Deployment里面的环境变量CALICO_IPV4POOL_CIDR为/etc/kubernetes/kubeadm-config.yaml里面networking.podSubnet的值。 文章只是在文章kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora的基础上，修改了master的HA方案。关于集群安装的详细步骤，建议访问kubeadm HA master(v1.13.0)离线包 + 自动化脚本 + 常用插件 For Centos/Fedora。 "},"container/kubernetes/Kubernetes集群安装Cert-Manager.html":{"url":"container/kubernetes/Kubernetes集群安装Cert-Manager.html","title":"Kubernetes集群安装Cert-Manager","keywords":"","body":"安装Cert Manager 一、Installing the Chart https://cert-manager.io/docs/installation/kubernetes/ 二、创建ClusterIssuer(集群内所有命名空间公用方案) apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: cluster-letsencrypt-prod spec: acme: email: hnbcao@qq.com privateKeySecretRef: name: cluster-letsencrypt-prod server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: traefik 三、创建Issuer(集群内单个命名空间独享方案) apiVersion: cert-manager.io/v1alpha2 kind: Issuer metadata: name: letsencrypt-prod spec: acme: email: hnbcao@qq.com privateKeySecretRef: name: letsencrypt-prod server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: traefik 四、Ingress应用ClusterIssuer kind: Ingress apiVersion: extensions/v1beta1 metadata: name: harbor-ingress namespace: ns-harbor labels: app: harbor chart: harbor heritage: Helm release: harbor annotations: cert-manager.io/cluster-issuer: cluster-letsencrypt-prod spec: tls: - hosts: - harbor.domian.io secretName: harbor-letsencrypt-tls rules: - host: harbor.domian.io http: paths: - path: / backend: serviceName: harbor-harbor-portal servicePort: 80 Ingress通过在annotations中添加cert-manager.io/cluster-issuer: cluster-letsencrypt-prod为ingress中的域名自动生成证书。 四、Ingress应用ClusterIssuer kind: Ingress apiVersion: extensions/v1beta1 metadata: name: harbor-ingress namespace: ns-harbor labels: app: harbor chart: harbor heritage: Helm release: harbor annotations: cert-manager.io/issuer: letsencrypt-prod spec: tls: - hosts: - harbor.domian.io secretName: harbor-letsencrypt-tls rules: - host: harbor.domian.io http: paths: - path: / backend: serviceName: harbor-harbor-portal servicePort: 80 Ingress通过在annotations中添加cert-manager.io/issuer: letsencrypt-prod为ingress中的域名自动生成证书。 结束 使用Cert Manager时，ingress中host配置的域名必须指定，不能有通配符； "},"container/kubernetes/Kubernetes集群安装Traefik-Ingress.html":{"url":"container/kubernetes/Kubernetes集群安装Traefik-Ingress.html","title":"Kubernetes集群安装Traefik-Ingress","keywords":"","body":"部署TraefikIngress 使用OpenSSL创建TLS证书（已有证书则跳过该选项） 设置证书信息 cd ~ && mkdir tls echo \"\"\" [req] distinguished_name = req_distinguished_name prompt = yes [ req_distinguished_name ] countryName = Country Name (2 letter code) countryName_value = CN stateOrProvinceName = State or Province Name (full name) stateOrProvinceName_value = Chongqing localityName = Locality Name (eg, city) localityName_value = Yubei organizationName = Organization Name (eg, company) organizationName_value = HNBCAO organizationalUnitName = Organizational Unit Name (eg, section) organizationalUnitName_value = R & D Department commonName = Common Name (eg, your name or your server\\'s hostname) commonName_value = *.hnbcao.io emailAddress = Email Address emailAddress_value = hnbcao@163.com \"\"\" > ~/tls/openssl.cnf 生成证书 openssl req -newkey rsa:4096 -nodes -config ~/tls/openssl.cnf -days 3650 -x509 -out ~/tls/tls.crt -keyout ~/tls/tls.key 部署Traefik 添加证书至集群 kubectl create -n kube-system secret tls ssl --cert ~/ikube/tls/tls.crt --key ~/ikube/tls/tls.key 部署Traefik kubectl apply -f https://raw.githubusercontent.com/hnbcao/kubeadm-ha/master/addons/yaml/traefik/traefik-daemonset-full.yaml "},"container/kubernetes/Kubernetes集群创建用户.html":{"url":"container/kubernetes/Kubernetes集群创建用户.html","title":"Kubernetes集群创建用户","keywords":"","body":"创建集群用户 1、创建用户 apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system 2、获取管理员用户的Token，通过执行如下命令获取系统Token信息 kubectl describe secret admin-user --namespace=kube-system 3、导入kubeconfig文件 DASH_TOCKEN=$(kubectl get secret -n kube-system admin-user-token-4j272 -o jsonpath={.data.token}|base64 -d) kubectl config set-cluster kubernetes --server=https://172.16.0.9:8443 --kubeconfig=/root/kube-admin.conf kubectl config set-credentials admin-user --token=$DASH_TOCKEN --kubeconfig=/root/kube-admin.conf kubectl config set-context admin-user@kubernetes --cluster=kubernetes --user=admin-user --kubeconfig=/root/kube-admin.conf kubectl config use-context admin-user@kubernetes --kubeconfig=/root/kube-admin.conf "},"container/kubernetes/Kubernetes集群创建Image-Pull-Secret.html":{"url":"container/kubernetes/Kubernetes集群创建Image-Pull-Secret.html","title":"Kubernetes集群创建Image-Pull-Secret","keywords":"","body":"创建ImagePullSecret 一、登录镜像仓库，成功之后会生成如下/root/.docker/config.json文件 { \"auths\": { \"harbor.hnbcao.tech\": { \"auth\": \"YWRtaW4******lRlY2g=\" } }, \"HttpHeaders\": { \"User-Agent\": \"Docker-Client/***\" } } 二、执行如下命令创建ImagePullSecret kubectl create secret generic harbor-admin-secret --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson --namespace hnbcao-mixing-ore 说明： harbor-admin-secret： ImagePullSecret名字 type： 指定secret类型为kubernetes.io/dockerconfigjson namespace：secret命名空间 四、为项目添加ImagePullSecret Deployment 在配置项的spec.template.spec.imagePullSecrets下添加secret：harbor-admin-secret。例如，Deployment的配置如下： kind: Deployment apiVersion: apps/v1 metadata: name: app-test spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: app-test app.kubernetes.io/name: hnbcao template: metadata: labels: app.kubernetes.io/instance: app-test app.kubernetes.io/name: hnbcao spec: containers: - name: hnbcao image: nginx imagePullSecrets: - name: harbor-admin-secret 结束 附上官网教程：https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ "},"container/kubernetes/Kubernetes集群跨namespace服务访问.html":{"url":"container/kubernetes/Kubernetes集群跨namespace服务访问.html","title":"Kubernetes集群跨namespace服务访问","keywords":"","body":"集群跨namespace服务访问 ns-02需要访问ns-01下面的服务service01 apiVersion: v1 kind: Service metadata: name: service02 namespace: ns-02 spec: ports: - name: http port: 80 protocol: TCP targetPort: 80 sessionAffinity: None type: ExternalName externalName: service01.ns-01.svc.cluster.local externalName：需要访问的服务域名，service01指服务名字，ns-01指命名空间，svc.cluster.local指kubernetes内部服务域名结尾，默认是svc.cluster.local "},"container/kubernetes/kubernetes集群运行问题记录.html":{"url":"container/kubernetes/kubernetes集群运行问题记录.html","title":"kubernetes集群运行问题记录","keywords":"","body":"kubernetes集群运行问题记录 集群内部容器无法解析外部DNS 原因：由于节点名为nodex.domain.xx，集群内部容器的/etc/resolv.conf中search域会domain.xx域，容器在解析外网域名时默认会先在外网域名后添加.domain.xx进行域名解析，而外网存在域名domain.xx，且DNS解析域名为.domain.xx，所以会将外网域名解析至.domain.xx对应的IP地址，最终导致容器内部无法访问外网域名。 解决办法：尽量避免集群内部节点名与外网域名冲突，可采用domain.local结尾等命名方式为节点命名。 gitlab迁移之后系统异常 原因：集群新建gitlab仓库各组件之间的认证文件与原有gialab不一致，导致恢复数据之后部分组件之间交互异常。 解决办法：新建gitlab之前，迁移原有gitlab中所有的secret文件。 kubernetes证书相关问题 原因：由于没有配置etcd证书的sans，导致集群master节点故障时，etcd无法启动，集群崩溃。 解决办法： 查看etcd日志，发现有出现关于证书的错误信息。master节点上执行openssl x509 -text -in /etc/kubernetes/pki/etcd/server.crt -noout查看证书的sans。输出证书信息为： Certificate: ... X509v3 extensions: ... X509v3 Subject Alternative Name: DNS:master1.segma.local, DNS:localhost, IP Address:192.168.1.202, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ... 其中X509v3 Subject Alternative Name项中，DNS和IP地址不包括其他主节点地址，所以证书不完整，需要重新生成证书。 以下所有操作在所有主节点上执行。首先备份/etc/kubernetes/pki下所有文件 cp -r /etc/kubernetes/pki /etc/kubernetes/pki_backup 使用kubeadm生成证书，新建kubeadm config文件，填写其他master节点信息。内容如下： cat > etcd-cert-conf.yaml 执行以下命令重新生成etcd证书： kubeadm init phase certs etcd-ca --config etcd-cert-conf.yaml kubeadm init phase certs etcd-server --config etcd-cert-conf.yaml kubeadm init phase certs etcd-peer --config etcd-cert-conf.yaml kubeadm init phase certs etcd-healthcheck-client --config etcd-cert-conf.yaml 更新证书：kubeadm alpha certs renew all "},"container/kubernetes/Kubernetes集群GPU共享解决方案.html":{"url":"container/kubernetes/Kubernetes集群GPU共享解决方案.html","title":"Kubernetes集群GPU共享解决方案","keywords":"","body":"Kubernetes集群GPU共享解决方案 基于阿里云的GPU共享方案 https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/docs/install.md "},"container/docker/":{"url":"container/docker/","title":"Docker","keywords":"","body":"Docker "},"container/docker/DockerCompose安装使用.html":{"url":"container/docker/DockerCompose安装使用.html","title":"DockerCompose安装使用","keywords":"","body":"Docker Compose安装使用 概述 Compose是用于定义和运行多容器Docker应用程序的工具。通过Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用一个命令，就可以从配置中创建并启动所有服务。 使用Compose基本上是一个三步过程： 打包应用镜像 使用docker-compose.yml文件定义应用服务 运行docker-compose up 启动服务 如下是一个redis服务的docker-compose.yml文件： version: '2.0' services: web: build: . ports: - \"5000:5000\" volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redis volumes: logvolume01: {} 概述 Compose是用于定义和运行多容器应用的工具。通过Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用一个命令，就可以从配置中创建并启动所有服务。例如：一个Wordpress项目包含mysql数据库和wordpress应用，首先创建docker-compose.yml文件（建议在wordpress文件夹下创建，方便管理），然后在docker-compose.yml文件所在的目录下运行“docker-compose up -d”，Compose就会通过配置创建并启动Wordpress。 一、安装Docker Compose 系统环境 Host Name OS IP master1 CentOS 7.5 192.168.56.114 通过如下命令下载Docker Compose # 下载docker-compose-1.25.4并保存至/usr/local/bin/目录下 sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # 修改docker-compose权限为可运行 sudo chmod +x /usr/local/bin/docker-compose 二、Compose常用命令简介 本文只介绍在生产环境中经常会用到的几个命令，关于其他命令，可通过“docker-compose help”命令查询，或者查询Docker官方网站 docker-compose up 应用启动/更新命令，构建、（重新）创建、启动并附加到服务的容器。“docker-compose up”命令后可跟参数， -d：后台运行应用，类似docker run -d； --force-recreate：强制重新生成容器； 其他命令可查询官方文档。 docker-compose down 停止应用所有容器并删除compose创建的网络，存储和容器。 三、docker-compose.yml介绍 下面是wordpress应用的docker-compose.yml的内容，各部分介绍将会通过该文件展开： version: '3.3' networks: app-tier: driver: bridge services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress networks: - app-tier wordpress: depends_on: - db image: wordpress:latest ports: - \"8000:80\" restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress networks: - app-tier volumes: db_data: {} services 配置应用运行的服务，wordpress应用配置了mysql数据库服务db和业务应用wordpress。服务配置说明如下： image：服务运行的Docker镜像； volumes：存储挂载，可挂载应用配置的volumes或者直接挂载宿主机路径,如：“- /opt/mysql/data:/var/lib/mysql” restart：服务重启策略。always：宕机自动重启； environment：环境变量配置； networks：服务网络配置； networks 配置应用运行的网络，服务通过networks指定镜像运行使用的网络。 volumes 配置存储，服务可通过volumes挂载。 "},"operations/CentOS离线镜像仓库创建.html":{"url":"operations/CentOS离线镜像仓库创建.html","title":"CentOS离线镜像仓库创建","keywords":"","body":"CentOS离线镜像仓库创建-以base仓库为例 一、安装相关软件 yum install createrepo reposync yum-utils -y 二、替换镜像源 备份 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 下载新的 CentOS-Base.repo 到 /etc/yum.repos.d/ Aliyun源地址为：https://developer.aliyun.com/mirror/ CentOS 7 wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo 或者 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo 三、同步镜像&创建本地仓库 # 新建文件夹，存储同步的仓库数据 mkdir -p /data/yum.repo && cd /data/yum.repo # 镜像仓库同步 reposync -r base -p ./ # 创建本地仓库 cd base && createrepo ./ 四、离线服务器使用 将同步的镜像仓库打包到离线服务器上，并解压至/mnt/yum.repo/base/下 备份 mkdir /etc/yum.repos.d/backup mv /etc/yum.repos.d/* /etc/yum.repos.d/backup 创建新的 repo 文件 到 /etc/yum.repos.d/ [base] name=CentOS-$releasever - Base - local-base-repo failovermethod=priority baseurl=file:///mnt/yum.repo/base/ gpgcheck=0 运行 yum makecache 生成缓存 五、定时同步脚本 该脚本同步kubernetes相关yum rpm源，定时同步使用crontab 加入该脚本即可，手动同步直接运行该脚本。脚本需要放置在本地保存yum源所在的目录。 #!/bin/bash BASE_DIR=$(pwd) echo \"base dir is ${BASE_DIR}\" repolist=(\"base\" \"docker-ce-stable\" \"extras\" \"kubernetes\" \"updates\") for repo in ${repolist[@]}; do echo \"sync repo $repo begin\" reposync -r ${repo} -p ${BASE_DIR} echo \"sync repo $repo end\" echo \"rebuild repo $repo begin\" createrepo --update ${BASE_DIR}/${repo} echo \"rebuild repo $repo end\" done "},"operations/Ceph集群安装.html":{"url":"operations/Ceph集群安装.html","title":"Ceph集群安装","keywords":"","body":"CephFS集群安装 一、集群规划 节点 ip os 节点说明 master1 10.73.13.61 centos7.4 mon+rgw+manger节点、ceph-deploy master2 10.73.13.60 centos7.4 mon+rgw+manger节点 master3 10.73.13.59 centos7.4 mon+rgw+manger节点 二、安装ceph-deploy Install and enable the Extra Packages for Enterprise Linux (EPEL) repository:Install and enable the Extra Packages for Enterprise Linux (EPEL) repository: sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 添加ceph仓库（阿里源，同时在所有节点进行如下操作） cat /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [ceph-source] name=Ceph source packages baseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/SRPMS enabled=1 EOM 安装python-setuptools、ceph-deploy sudo yum install python-setuptools sudo yum install ceph-deploy 三、节点准备 以下操作需在所有节点进行。 免密登录 # 1、三次回车后，密钥生成完成 ssh-keygen # 2、拷贝密钥到其他节点 ssh-copy-id -i ~/.ssh/id_rsa.pub 用户名字@192.168.x.xxx 时间同步 网上教程很多，具体可参考网上文档。 sudo yum install ntpdate ntpdate -u ntp.api.bz 关闭SELinux、防火墙 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 关闭系统的Swap（Kubernetes 1.8开始要求） swapoff -a yes | cp /etc/fstab /etc/fstab_bak cat /etc/fstab_bak |grep -v swap > /etc/fstab 四、集群安装 以下操作在ceph-deploy节点。 创建集群 mkdir /etc/ceph && cd /etc/ceph # ceph-deploy new {initial-monitor-node(s)} ceph-deploy new master1 master2 master3 设置集群配置 具体集群配置说明，待后续更新 cat /etc/ceph/ceph.conf [global] public_network = 10.73.13.0/16 cluster_network = 10.73.13.0/16 fsid = 464c2aa2-7426-4d6b-a0ae-961d1589ee53 mon_initial_members = master1, master2, master3 mon_host = 10.73.13.61,10.73.13.60,10.73.13.59 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd_pool_default_size = 3 osd_pool_default_min_size = 1 osd_pool_default_pg_num = 8 osd_pool_default_pgp_num = 8 osd_crush_chooseleaf_type = 1 [mon] mon_clock_drift_allowed = 0.5 mon allow pool delete = true [osd] osd_mkfs_type = xfs osd_mkfs_options_xfs = -f filestore_max_sync_interval = 5 filestore_min_sync_interval = 0.1 filestore_fd_cache_size = 655350 filestore_omap_header_cache_size = 655350 filestore_fd_cache_random = true osd op threads = 8 osd disk threads = 4 filestore op threads = 8 max_open_files = 655350 [mgr] mgr modules = dashboard 为所有节点安装ceph # ceph-deploy install {initial-monitor-node(s)} --no-adjust-repos # --no-adjust-repos参数的意思是不更新节点配置的ceph源，因为在安装ceph-deploy的步骤下已经为节点配置了阿里云的ceph源 ceph-deploy new master1 master2 master3 --no-adjust-repos 初始化节点配置,生产相应的keys ceph-deploy mon create-initial 完成之后会在/etc/ceph目录下生成以下几个文件 ceph.client.admin.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-mds.keyring ceph.bootstrap-rgw.keyring ceph.bootstrap-rbd.keyring ceph.bootstrap-rbd-mirror.keyring 拷贝文件至部署节点 ceph-deploy admin master1 master2 master3 部署mgr ceph-deploy mgr create master1 master2 master3 添加OSD ceph-deploy osd create --data /dev/vdb master1 ceph-deploy osd create --data /dev/vdb master2 ceph-deploy osd create --data /dev/vdb master3 五、创建CephFS文件系统 部署metadata服务 ceph-deploy mds create master1 master2 master3 生成CephFS ceph osd pool create cephfs_data 128 ceph osd pool create cephfs_meta 128 ceph fs new mycephfs cephfs_meta cephfs_data "},"operations/Fluent-bit日志插件配置说明.html":{"url":"operations/Fluent-bit日志插件配置说明.html","title":"Fluent-bit日志插件配置说明","keywords":"","body":"Fluent-bit日志插件配置说明 一、概述 fluent-bit配置文件中，主要由输入（Input）、解析器（Parser）、过滤器（Filter）、缓存（Buffer）、路由（Routing）、输出（Output）六大模块组成，各个模块的详细说明如下： Interface Description(英文) Description(中文) Input Entry point of data. Implemented through Input Plugins, this interface allows to gather or receive data. E.g: log file content, data over TCP, built-in metrics, etc. 数据的入口点。通过输入插件实现，此接口允许收集或接收数据。例如：日志文件内容，TCP上的数据，内置指标等。 Parser Parsers allow to convert unstructured data gathered from the Input interface into a structured one. Parsers are optional and depends on Input plugins. 解析器允许将从Input接口收集的非结构化数据转换为结构化数据。解析器是可选的，并且取决于Input插件。 Filter The filtering mechanism allows to alter the data ingested by the Input plugins. Filters are implemented as plugins. 过滤机制允许更改 Input插件提取的数据。过滤器被实现为插件。 Buffer By default, the data ingested by the Input plugins, resides in memory until is routed and delivered to an Output interface. 默认情况下，Input插件提取的数据将驻留在内存中，直到路由并传递到Output接口为止。 Routing Data ingested by an Input interface is tagged, that means that a Tag is assigned and this one is used to determinate where the data should be routed based on a match rule. Input接口摄取的数据被标记，这意味着分配了一个Tag，并且该标记用于根据匹配规则确定应将数据路由到的位置。 Output An output defines a destination for the data. Destinations are handled by output plugins. Note that thanks to the Routing interface, the data can be delivered to multiple destinations. 输出定义数据的目的地。目的地由输出插件处理。请注意，借助“路由”接口，可以将数据传递到多个目的地。 二、配置文件 一个fluent-bit配置文件除了包括输入（Input）、解析器（Parser）、过滤器（Filter）、缓存（Buffer）、路由（Routing）、输出（Output）六个模块外，还需要配置Service，该模块主要负责fluent-bit的配置，如Parser配置文件路径、fluent-bit自身日志打印等。如下是fluent-bit.conf配置： [SERVICE] Flush 1 Daemon Off Log_Level debug Parsers_File parsers.conf [INPUT] Name tail Path ${K8S_LOG_DIR}/*.log Parser json Tag kube_file.* Refresh_Interval 5 Mem_Buf_Limit 5MB Skip_Long_Lines OFF [FILTER] Name record_modifier Match kube_file.* Record hostname ${K8S_HOSTNAME} Record namespace ${K8S_POD_NAMESPACE} Record application ${K8S_APPLICATION_NAME} Record pod ${K8S_POD_NAME} Record container ${K8S_CONTAINER_NAME} Record node ${K8S_NODE_NAME} [OUTPUT] Name es Match * Host ${ELASTICSEARCH_HOST} Port ${ELASTICSEARCH_PORT} Logstash_Format On Retry_Limit False Type flb_type Time_Key time Time_Key_Format yyyy-MM-dd HH:mm:ss.SSS Replace_Dots On Logstash_Prefix segma_application_file 三、Service Service各个配置项如下： Key Description 中文描述 Default Value Flush Set the flush time in seconds. Everytime it timeouts, the engine will flush the records to the output plugin. 设置Flush时间（以秒为单位）。每次超时，引擎都会将记录刷新到输出插件。 5 Daemon Boolean value to set if Fluent Bit should run as a Daemon (background) or not. Allowed values are: yes, no, on and off. 一个布尔值，用于设置Fluent Bit是否应作为守护程序（后台）运行。允许的值为：是，否，打开和关闭。 Off Log_File Absolute path for an optional log file. 可选日志文件的绝对路径。 - Log_Level Set the logging verbosity level. Allowed values are: error, info, debug and trace. Values are accumulative, e.g: if 'debug' is set, it will include error, info and debug. Note that trace mode is only available if Fluent Bit was built with the WITH_TRACE option enabled. 设置日志记录的详细程度。允许的值为：error, info, debug 和 trace。值是累积值，例如：如果设置了“ debug”，则它将包括error, info 和 debug。请注意，只有在启用WITH_TRACE选项的情况下构建Fluent Bit时，跟踪模式才可用。 info Parsers_File Path for a parsers configuration file. Multiple Parsers_File entries can be used. 配置文件的路径。可以使用多个Parsers_File条目。 - HTTP_Server Enable built-in HTTP Server 启用内置的HTTP服务器 Off HTTP_Listen Set listening interface for HTTP Server when it's enabled 启用HTTP Server时设置监听接口 0.0.0.0 HTTP_Port Set TCP Port for the HTTP Server 设置HTTP服务器的TCP端口 2020 在Kubernetes中进行日志收集时，一般需要配置的项有：Flush、Daemon、Log_Level、Parsers_File。在调试插件时，关闭Daemon以及设置Log_Level为debug甚至更高可方便查看插件运行错误。Parsers_File对应解析器的配置，可参考官方提供的parsers.conf 四、Input INPUT模块指点了日志输入源，每个输入插件都可以添加自己的配置键。以下为目前官方支持的输入源插件,官方Input Plugins。运行在kubernetes集群内的应用日志收集时，我们使用的是tail插件常用参数如下： Key Description 中文描述 Default Buffer_Chunk_Size Set the initial buffer size to read files data. This value is used too to increase buffer size. The value must be according to the Unit Size specification. 设置初始缓冲区大小以读取文件数据。该值也用于增加缓冲区大小。该值必须符合“ 单位大小”规范。 32k Buffer_Max_Size Set the limit of the buffer size per monitored file. When a buffer needs to be increased (e.g: very long lines), this value is used to restrict how much the memory buffer can grow. If reading a file exceed this limit, the file is removed from the monitored file list. The value must be according to the Unit Size specification. 设置每个受监视文件的缓冲区大小的限制。当需要增加缓冲区时（例如：很长的行），该值用于限制内存缓冲区可以增长多少。如果读取的文件超过此限制，将从监视的文件列表中删除该文件。该值必须符合“ 单位大小”规范。 Buffer_Chunk_Size Path Pattern specifying a specific log files or multiple ones through the use of common wildcards. 通过使用通用通配符指定一个或多个特定日志文件的模式。 Path_Key If enabled, it appends the name of the monitored file as part of the record. The value assigned becomes the key in the map. 如果启用，它将附加受监视文件的名称作为记录的一部分。分配的值成为映射中的键。 Exclude_Path Set one or multiple shell patterns separated by commas to exclude files matching a certain criteria, e.g: exclude_path=.gz,.zip 设置一个或多个用逗号分隔的外壳模式，以排除符合特定条件的文件，例如：exclude_path = .gz，.zip Refresh_Interval The interval of refreshing the list of watched files. Default is 60 seconds. 刷新监视文件列表的时间间隔。默认值为60秒。 Rotate_Wait Specify the number of extra seconds to monitor a file once is rotated in case some pending data is flushed. Default is 5 seconds. 指定在刷新某些未决数据时旋转一次后监视文件的额外秒数。默认值为5秒。 Skip_Long_Lines When a monitored file reach it buffer capacity due to a very long line (Buffer_Max_Size), the default behavior is to stop monitoring that file. Skip_Long_Lines alter that behavior and instruct Fluent Bit to skip long lines and continue processing other lines that fits into the buffer size. 当受监视的文件由于行很长（Buffer_Max_Size）而达到缓冲区容量时，默认行为是停止监视该文件。Skip_Long_Lines会更改该行为，并指示Fluent Bit跳过长行并继续处理适合缓冲区大小的其他行。 Off DB Specify the database file to keep track of monitored files and offsets. 指定数据库文件以跟踪受监视的文件和偏移量。 DB.Sync Set a default synchronization (I/O) method. Values: Extra, Full, Normal, Off. This flag affects how the internal SQLite engine do synchronization to disk, for more details about each option please refer to this section. 设置默认的同步（I / O）方法。值：Extra，Full，Normal，Off。此标志影响内部SQLite引擎与磁盘同步的方式，有关每个选项的更多详细信息，请参阅本节。 Full Mem_Buf_Limit Set a limit of memory that Tail plugin can use when appending data to the Engine. If the limit is reach, it will be paused; when the data is flushed it resumes. 设置将数据附加到引擎时，Tail插件可以使用的内存限制。如果达到极限，它将被暂停；刷新数据后，它将恢复。 Parser Specify the name of a parser to interpret the entry as a structured message. 指定解析器的名称，以将条目解释为结构化消息。 Key When a message is unstructured (no parser applied), it's appended as a string under the key name log. This option allows to define an alternative name for that key. 当消息是非结构化消息（未应用解析器）时，它将作为字符串附加在键名log下。此选项允许为该键定义替代名称。 log 常用的参数有Path、Parser、Tag、Refresh_Interval、Mem_Buf_Limit、Skip_Long_Lines。Path指定日志所在目录；Tag为Routing参数，会在Routing章节介绍；Refresh_Interval指定日志刷新时间间隔，会直接影响日志收集的实时性，一般设置为5秒；Mem_Buf_Limit指定插件内存大小，一般设置5Mb;Skip_Long_Lines一般使用默认值Off；其中比较重要的是Parser参数的配置，主要匹配Parser解析器，其值为解析器的name值，下一章介绍Parser解析器。 五、Parser Parser解析器的配置是单独的一个配置文件，配置的加载是由Service模块的Parsers_File参数指定。可参考官方提供的parsers.conf 六、Filter Filter插件允许改变输入数据的结构，例如在Kubernetes集群中，我们需要日志记录应用的名字以及所在节点，而默认日志未打印该值，我们便可以使用Filter附加该值。详情见官方插件信息 七、Buffer 当准备好将数据或日志路由到某个目标位置时，默认情况下会将它们缓冲在内存中。 八、Routing 路由是一项核心功能，可让您通过过滤器将数据路由到一个或多个目的地。路由主要由两个参数控制。 Tag：当数据由输入插件生成时，它附带一个标签（大多数情况下是手动配置该标签），该标签是人类可读的指示器，有助于识别数据源。 Match：我们定义其中的数据应被路由，一个匹配规则在配置中进行分配。 下面文件将简单介绍路由规则的实现： [INPUT] Name cpu Tag my_cpu [INPUT] Name mem Tag my_mem [OUTPUT] Name es Match my_cpu [OUTPUT] Name stdout Match my_mem 在上面的配置中，es输出通过Match值my_cpu匹配到输入插件cpu的Tag值，而stdout输出则通过Match值my_mem匹配到输入插件mem的Tag值，所以es输出只接收cpu输出，stdout输出只接收mem输出。 九、Output 输出接口允许定义数据的目的地。通用目标是远程服务，本地文件系统或其他标准接口。输出实现为插件，并且有很多可用的插件。 有关更多详细信息，请参阅“输出插件”部分。 Filter 附录 Fluent-bit官方文档 https://fluentbit.io/documentation "},"operations/FTP服务器搭建.html":{"url":"operations/FTP服务器搭建.html","title":"FTP服务器搭建","keywords":"","body":"FTP服务搭建 系统： centos 7.4 一、安装vsftpd yum -y install vsftpd 二、配置服务 [root@ecs-7fd0 vsftpd]# cat /etc/vsftpd/vsftpd.conf anonymous_enable=NO local_enable=YES write_enable=YES local_umask=022 dirmessage_enable=YES xferlog_enable=YES xferlog_std_format=YES ascii_upload_enable=YES ascii_download_enable=YES chroot_local_user=YES listen=NO listen_ipv6=YES connect_from_port_20=NO #设置使用主动模式 pasv_enable=YES pasv_min_port=1024 pasv_max_port=65536 pam_service_name=vsftpd guest_enable=YES #设置使用虚拟用户的真实访问用户 guest_username=ftpuser user_config_dir=/etc/vsftpd/vsftpd_user_conf allow_writeable_chroot=YES #设置使用虚拟用户 virtual_use_local_privs=YES userlist_enable=YES userlist_deny=NO tcp_wrappers=YES 三、创建ftpuser账户 useradd -d /home/ftpuser -s /sbin/nologin ftpuser 三、虚拟用户 设置pam策略 [root@ecs-7fd0 vsftpd]# cat /etc/pam.d/vsftpd #%PAM-1.0 #session optional pam_keyinit.so force revoke #auth required pam_listfile.so item=user sense=deny file=/etc/vsftpd/ftpusers onerr=succeed #auth required pam_shells.so #auth include password-auth #account include password-auth #session required pam_loginuid.so #session include password-auth auth required pam_userdb.so db=/etc/vsftpd/vsftpd_login account required pam_userdb.so db=/etc/vsftpd/vsftpd_login 配置虚拟账户 [root@ecs-7fd0 vsftpd]# cat ftp_virtual_user User001 PasswordForUser001 User002 PasswordForUser002 User003 PasswordForUser003 生成虚拟账号数据库 [root@ecs-7fd0 vsftpd]# db_load -T -t hash -f /etc/vsftpd/ftp_virtual_user /etc/vsftpd/vsftpd_login.db 配置虚拟账户访问目录 [root@ecs-7fd0 vsftpd]# mkdir -p /etc/vsftpd/vsftpd_user_conf [root@ecs-7fd0 vsftpd]# cat User001 local_root=/home/ftpuser/User001 write_enable=YES anon_world_readable_only=YES anon_upload_enable=YES anon_mkdir_write_enable=YES anon_other_write_enable=YES [root@ecs-7fd0 vsftpd]# cat User002 local_root=/home/ftpuser/User002 write_enable=YES anon_world_readable_only=YES anon_upload_enable=YES anon_mkdir_write_enable=YES anon_other_write_enable=YES [root@ecs-7fd0 vsftpd]# cat User003 local_root=/home/ftpuser/User003 write_enable=YES anon_world_readable_only=YES anon_upload_enable=YES anon_mkdir_write_enable=YES anon_other_write_enable=YES 启动vsftpd [root@ecs-7fd0 vsftpd]# systemctl enable vsftpd [root@ecs-7fd0 vsftpd]# systemctl start vsftpd "},"software_engineering/设计模式简介.html":{"url":"software_engineering/设计模式简介.html","title":"设计模式简介","keywords":"","body":"设计模式简介 设计模式原则 开闭原则：开闭原则（Open Closed Principle，OCP）由勃兰特·梅耶（Bertrand Meyer）提出，他在 1988 年的著作《面向对象软件构造》（Object Oriented Software Construction）中提出：软件实体应当对扩展开放，对修改关闭（Software entities should be open for extension，but closed for modification），这就是开闭原则的经典定义。 里氏替换原则：里氏替换原则（Liskov Substitution Principle，LSP）由麻省理工学院计算机科学实验室的里斯科夫（Liskov）女士在 1987 年的“面向对象技术的高峰会议”（OOPSLA）上发表的一篇文章《数据抽象和层次》（Data Abstraction and Hierarchy）里提出来的，她提出：继承必须确保超类所拥有的性质在子类中仍然成立（Inheritance should ensure that any property proved about supertype objects also holds for subtype objects）。 依赖倒置原则：依赖倒置原则（Dependence Inversion Principle，DIP）的原始定义为，高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象（High level modules shouldnot depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details. Details should depend upon abstractions）。其核心思想是：要面向接口编程，不要面向实现编程。 单一职责原则：单一职责原则（Single Responsibility Principle，SRP）又称单一功能原则，由罗伯特·C.马丁（Robert C. Martin）于《敏捷软件开发：原则、模式和实践》一书中提出的。这里的职责是指类变化的原因，单一职责原则规定一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分（There should never be more than one reason for a class to change）。 接口隔离原则：接口隔离原则（Interface Segregation Principle，ISP）要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。 迪米特法则：迪米特法则（Law of Demeter，LoD）又叫作最少知识原则（Least Knowledge Principle，LKP)，迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话（Talk only to your immediate friends and not to strangers）。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 合成复用原则：合成复用原则（Composite Reuse Principle，CRP）又叫组合/聚合复用原则（Composition/Aggregate Reuse Principle，CARP）。它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。 设计模式分类 根据目的来分 根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式 3 种。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 根据作用范围来分 根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。 表 1 介绍了这 23 种设计模式的分类。 | 范围\\目的 | 创建型模式 | 结构型模式 | 行为型模式 | | :-: | - | - | - | | 类模式 | 工厂方法 (类） | 适配器 | 模板方法、解释器 | | 对象模式 | 单例原型抽象工厂建造者 | 代理(对象）适配器桥接装饰外观享元组合 | 策略命令职责链状态观察者中介者迭代器访问者备忘录 | GoF的23种设计模式的功能 前面说明了 GoF 的 23 种设计模式的分类，现在对各个模式的功能进行介绍。 创建型模式(Creational)： 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 结构型模式(Structural)： 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。数据库连接池 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 行为型模式(Behavioral)： 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 设计模式使用频次总结 创建型模式(Creational) 高频： 工厂方法模式(Factory Method ) 、抽象工厂模式(Abstract Factory ) 、单例模式(Singleton)、建 造者模式(Builder) 低频 ： 原型模式( Prototype ) 结构型模式(Structural) 高频： 代理模式(Proxy ) 、门面模式(Facade ) 、装饰器模式(Decorator) 、享元模式(Flyweight) 、适配器模式(Adapter)、组合模式(Composite ) 低频 ： 桥接模式( Bridge ) 行为型模式(Behavioral) 高频： 模板方法模式(Template Method ) 、策略模式(Strategy ) 、 责任链模式(Chain of Responsibility ) 、状态模式(State ) 低频： 备忘录模式(Memento ) 、 观察者模式(Observer) s 迭代器模式(Iterator) s 中介者模式(Mediator)、命令模式(Command ) 、解释器模式( Interpreter) 、访问者模式(Visitor) 模式对比 创建型模式： 工厂方法模式(Factory Method ) 抽象工厂模式(Abstract Factory ) 单例模式(Singleton) 建造者模式(Builder) 结构型模式： 代理模式(Proxy ) 门面模式(Facade ) 装饰器模式(Decorator) 享元模式(Flyweight) 适配器模式(Adapter) 组合模式(Composite ) 行为型模式： 模板方法模式(Template Method ) 策略模式(Strategy ) 责任链模式(Chain of Responsibility ) 状态模式(State ) "}}